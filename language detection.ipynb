{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "import string\n",
    "from pyspark.ml.feature import HashingTF, IDF, Word2Vec\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import timeit\n",
    "import sys\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression, DecisionTreeClassifier,RandomForestClassifier,LogisticRegressionModel\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    punctuations = list(string.punctuation)\n",
    "    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', 'â€“']\n",
    "    punctuations += extra_punctuations\n",
    "    filtered = [w for w in sentence.lower() if w not in punctuations]\n",
    "    return (\"\".join(filtered)).split()\n",
    "\n",
    "def clean_data(file):\n",
    "    data = sc.textFile(file)\n",
    "    col_rdd = data.map(lambda x: (x.split('\\t')[0], x[-1]))\n",
    "    punctuation_removed_rdd = col_rdd.map(lambda x: (remove_punctuation(x[0]), float(x[1])))\n",
    "    data_df = sqlContext.createDataFrame(punctuation_removed_rdd, [\"text\", \"label\"])\n",
    "    \n",
    "    remover = StopWordsRemover(inputCol=\"text\", outputCol=\"words\", stopWords=stopwords.words('english'))\n",
    "    a = remover.transform(data_df).select([\"label\", \"words\"])\n",
    "    return a\n",
    "\n",
    "def tf_idf(data_rdd_df):\n",
    "    #data_rdd_df = data_rdd_df.toDF()  #use it when random split\n",
    "    hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"tf_features\")\n",
    "    tf_data = hashing_tf.transform(data_rdd_df)\n",
    "\n",
    "    idf_data = IDF(inputCol=\"tf_features\", outputCol=\"features\").fit(tf_data)\n",
    "    tf_idf_data = idf_data.transform(tf_data)\n",
    "    return tf_idf_data.select([\"label\", \"words\", \"features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset:  160726\n",
      "Size of test dataset:  101189\n",
      " LR Time:  25.300158759000624\n"
     ]
    }
   ],
   "source": [
    "#detectionMultil.txt : contains tweets in en, fr, gr \n",
    "# + sentences in these three languages (http://downloads.tatoeba.org/exports/sentences.tar.bz2)\n",
    "\n",
    "#filtered_data_df = clean_data(\"data/detectionMultil.txt\")\n",
    "#training, test = filtered_data_df.rdd.randomSplit([0.7, 0.3], seed=0)\n",
    "\n",
    "training = clean_data(\"data/detectionMultil.txt\")\n",
    "test = clean_data(\"data/detectionTest.txt\")\n",
    "print(\"Size of train dataset: \",training.count())\n",
    "print(\"Size of test dataset: \",test.count())\n",
    "\n",
    "train_df = tf_idf(training)\n",
    "test_df = tf_idf(test)\n",
    "\n",
    "#train the first time and save\n",
    "lor = LogisticRegression(regParam=0.01)\n",
    "start = timeit.default_timer()\n",
    "model = lor.fit(train_df)\n",
    "stop = timeit.default_timer()\n",
    "print(' LR Time: ', stop - start)\n",
    "# model.save(\"lr_LangModel.model\")\n",
    "\n",
    "#then just load predicted model\n",
    "# model = LogisticRegressionModel.load(\"lr_LangModel.model\")\n",
    "#lor_predicted_df = model.transform(test_df).select([\"label\", \"words\", \"prediction\"])\n",
    "lor_predicted_df = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8915494767217781\n",
      "------------\n",
      "Actual number of tweets\n",
      "greek:  33393\n",
      "french:  33896\n",
      "english:  33900\n",
      "------------\n",
      "Predicted number of tweets\n",
      "greek:  35161\n",
      "french:  25526\n",
      "english:  40502\n",
      "------------\n",
      "Lost =  10974\n"
     ]
    }
   ],
   "source": [
    "accuracy = 1.0 * lor_predicted_df.filter(lor_predicted_df.label == lor_predicted_df.prediction).count() / lor_predicted_df.count()\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(\"------------\")\n",
    "greek = lor_predicted_df.filter((lor_predicted_df.label == 1.0) ).count()\n",
    "french = lor_predicted_df.filter((lor_predicted_df.label == 2.0) ).count()\n",
    "english = lor_predicted_df.filter((lor_predicted_df.label == 3.0) ).count()\n",
    "print(\"Actual number of tweets\")\n",
    "print(\"greek: \", greek)\n",
    "print(\"french: \", french)\n",
    "print(\"english: \", english)\n",
    "print(\"------------\")\n",
    "\n",
    "greek2 = lor_predicted_df.filter((lor_predicted_df.prediction == 1.0)).count()\n",
    "french2 = lor_predicted_df.filter((lor_predicted_df.prediction == 2.0)).count()\n",
    "english2 = lor_predicted_df.filter((lor_predicted_df.prediction == 3.0)).count()\n",
    "print(\"Predicted number of tweets\")\n",
    "print(\"greek: \", greek2)\n",
    "print(\"french: \", french2)\n",
    "print(\"english: \", english2)\n",
    "print(\"------------\")\n",
    "\n",
    "lost = lor_predicted_df.filter((lor_predicted_df.prediction != lor_predicted_df.label)).count()\n",
    "print(\"Lost = \", lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
